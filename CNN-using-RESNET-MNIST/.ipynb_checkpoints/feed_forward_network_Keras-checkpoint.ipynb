{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from resnet164 import ResNet164\n",
    "#from utils import load_mnist\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten, Lambda \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from scipy.ndimage.interpolation import rotate, shift, zoom\n",
    "from keras.constraints import max_norm\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction, true_label):\n",
    "    pred_indices = np.argmax(prediction, 1)\n",
    "    true_indices = np.argmax(true_label, 1)\n",
    "    return np.mean(pred_indices == true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOFTMAX(s_):\n",
    "    return np.exp(s_) / np.matmul(np.ones((1, s_.shape[0])), np.exp(s_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss(y_true, y_pred, alpha):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_true_softs = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss =(alpha*tf.keras.losses.categorical_crossentropy(y_true,y_pred) +\n",
    "           tf.keras.losses.categorical_crossentropy(y_true_softs, y_pred_softs))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return tf.keras.metrics.categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_jitter(temp):\n",
    "    if np.random.random() > .7:\n",
    "        temp = rotate(temp, angle = np.random.randint(-25, 25), reshape=False)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1).astype('float32')\n",
    "X_test = X_test.reshape(10000,28,28,1).astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train a large NN with two hidden layers of 1200 ReLu hidden units on all training examples. The net has to be regularized using dropout and weight-constraint. Also, the images were jittered up to two pixels in any direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#teacher model:\n",
    "teacher = Sequential()\n",
    "teacher.add(Flatten(input_shape=(28,28,1)))\n",
    "teacher.add(Dense(1200, kernel_constraint=max_norm(4.), activation='relu'))\n",
    "teacher.add(Dense(1200, kernel_constraint=max_norm(4.), activation='relu'))\n",
    "teacher.add(Dropout(.5))\n",
    "teacher.add(Dense(10))\n",
    "teacher.add(Activation('softmax'))\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "# print(teacher.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/user/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 1.1643 - acc: 0.6983 - val_loss: 0.5137 - val_acc: 0.8814\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.5346 - acc: 0.8500 - val_loss: 0.3643 - val_acc: 0.9046\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.4369 - acc: 0.8742 - val_loss: 0.3164 - val_acc: 0.9124\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.3883 - acc: 0.8877 - val_loss: 0.2860 - val_acc: 0.9210\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 16s 272us/step - loss: 0.3585 - acc: 0.8963 - val_loss: 0.2639 - val_acc: 0.9260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124e34fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_temp = np.copy(X_train) # Copy to not effect the originals\n",
    "#Add noise 'jitter':\n",
    "for j in range(0, X_train_temp.shape[0]):\n",
    "    X_train_temp[j, :, :,0] = rand_jitter(X_train_temp[j,:,:,0])\n",
    "\n",
    "teacher.fit(X_train_temp, y_train, batch_size=128, epochs=5, verbose=1, validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train a smaller network with two hidden layers of 800 ReLu hidden units without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller teacher model:\n",
    "small_teacher = Sequential()\n",
    "small_teacher.add(Flatten(input_shape=(28,28,1)))\n",
    "small_teacher.add(Dense(800, activation='relu'))\n",
    "small_teacher.add(Dense(800, activation='relu'))\n",
    "small_teacher.add(Dense(10, activation='softmax'))\n",
    "\n",
    "small_teacher.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "#print(teacher.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 1.1072 - acc: 0.7548 - val_loss: 0.5113 - val_acc: 0.8783\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.4888 - acc: 0.8693 - val_loss: 0.3662 - val_acc: 0.9028\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.4000 - acc: 0.8876 - val_loss: 0.3165 - val_acc: 0.9126\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3591 - acc: 0.8972 - val_loss: 0.2889 - val_acc: 0.9196\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.3314 - acc: 0.9062 - val_loss: 0.2692 - val_acc: 0.9255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ee8cef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_temp = np.copy(X_train) # Copy to not effect the originals\n",
    "#Add noise:\n",
    "for j in range(0, X_train_temp.shape[0]):\n",
    "    X_train_temp[j, :, :,0] = rand_jitter(X_train_temp[j,:,:,0])\n",
    "\n",
    "small_teacher.fit(X_train_temp, y_train, batch_size=128, epochs=5, verbose=1, validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add regularization to this small network to make it match the soft targets produced by the large net at a temperature of 20. **In the paper this network achieves 74 test errors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we now use the small_teacher model as the student model:\n",
    "# Raise the temperature of teacher model and gather the soft targets\n",
    "# Set a tempature value\n",
    "temp = 20\n",
    "#Collect the logits from the previous layer output and store it in a different model\n",
    "teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('dense_3').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "#student model:\n",
    "student_m = Sequential()\n",
    "student_m.add(Flatten(input_shape=(28,28,1)))\n",
    "student_m.add(Dense(800, activation='relu'))\n",
    "student_m.add(Dense(800, activation='relu'))\n",
    "student_m.add(Dense(10))\n",
    "#student_m.add(Activation('softmax'))\n",
    "student_m.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "#print(student_m.summary())\n",
    "\n",
    "logits = student_m.layers[-1].output\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "student_m = Model(student_m.input, output)\n",
    "\n",
    "student_m.compile(optimizer='SGD',\n",
    "                      loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 1),\n",
    "                      metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_train_logits = teacher_WO_Softmax.predict(X_train)\n",
    "teacher_test_logits = teacher_WO_Softmax.predict(X_test)\n",
    "\n",
    "Y_train_soft = SOFTMAX(teacher_train_logits/temp)\n",
    "Y_test_soft = SOFTMAX(teacher_test_logits/temp)\n",
    "\n",
    "Y_train_new = np.concatenate([y_train, Y_train_soft], axis=1)\n",
    "Y_test_new =  np.concatenate([y_test, Y_test_soft], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 1.0872 - acc: 0.7747 - val_loss: 0.5114 - val_acc: 0.8819\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.4400 - acc: 0.8869 - val_loss: 0.3644 - val_acc: 0.9037\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3522 - acc: 0.9036 - val_loss: 0.3132 - val_acc: 0.9148\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.3128 - acc: 0.9126 - val_loss: 0.2862 - val_acc: 0.9218\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2872 - acc: 0.9192 - val_loss: 0.2672 - val_acc: 0.9264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10319eba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_m.fit(X_train, Y_train_new,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 4\n",
    "nb_classes = 10\n",
    "#student model:\n",
    "student_m2 = Sequential()\n",
    "student_m2.add(Flatten(input_shape=(28,28,1)))\n",
    "student_m2.add(Dense(30, activation='relu'))\n",
    "student_m2.add(Dense(30, activation='relu'))\n",
    "student_m2.add(Dense(10))\n",
    "#student_m.add(Activation('softmax'))\n",
    "student_m2.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "#print(student_m2.summary())\n",
    "\n",
    "logits = student_m2.layers[-1].output\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "student_m2 = Model(student_m2.input, output)\n",
    "\n",
    "student_m2.compile(optimizer='SGD',\n",
    "                      loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 1),\n",
    "                      metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.7625 - acc: 0.4540 - val_loss: 1.0378 - val_acc: 0.7441\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.7327 - acc: 0.8145 - val_loss: 0.5308 - val_acc: 0.8605\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.4856 - acc: 0.8673 - val_loss: 0.4092 - val_acc: 0.8890\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.4008 - acc: 0.8890 - val_loss: 0.3544 - val_acc: 0.9016\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.3559 - acc: 0.9003 - val_loss: 0.3225 - val_acc: 0.9093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138430668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_m2.fit(X_train, Y_train_new,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "#student model:\n",
    "student_m3 = Sequential()\n",
    "student_m3.add(Flatten(input_shape=(28,28,1)))\n",
    "student_m3.add(Dense(800, activation='relu'))\n",
    "student_m3.add(Dense(800, activation='relu'))\n",
    "student_m3.add(Dense(10))\n",
    "\n",
    "student_m3.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "# print(student_m3.summary())\n",
    "\n",
    "logits = student_m3.layers[-1].output\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "student_m3 = Model(student_m3.input, output)\n",
    "\n",
    "student_m3.compile(optimizer='SGD',\n",
    "                      loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 1),\n",
    "                      metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes_idx = np.where(y_train[:,2] == 1)[0]\n",
    "non_threes_idx = np.where(y_train[:,2] == 0)[0]\n",
    "threes_n_examples = len(threes_idx)\n",
    "\n",
    "new_training_X = X_train[non_threes_idx]\n",
    "teacher_train_logits = teacher_WO_Softmax.predict(new_training_X)\n",
    "Y_train_soft = SOFTMAX(teacher_train_logits/temp)\n",
    "Y_train_new = np.concatenate([y_train[non_threes_idx], Y_train_soft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54042/54042 [==============================] - 7s 123us/step - loss: 1.0094 - acc: 0.7941\n",
      "Epoch 2/5\n",
      "54042/54042 [==============================] - 6s 110us/step - loss: 0.4016 - acc: 0.8966\n",
      "Epoch 3/5\n",
      "54042/54042 [==============================] - 6s 111us/step - loss: 0.3169 - acc: 0.9122\n",
      "Epoch 4/5\n",
      "54042/54042 [==============================] - 6s 110us/step - loss: 0.2796 - acc: 0.9204\n",
      "Epoch 5/5\n",
      "54042/54042 [==============================] - 6s 110us/step - loss: 0.2561 - acc: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139cc2ef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_m3.fit(new_training_X, Y_train_new,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excluded_label_pred(dif_matrix, label_idx_list):\n",
    "    indexes = []\n",
    "    wrong_excluded_lable_pred = []\n",
    "\n",
    "    for idx, i in enumerate(dif_matrix):\n",
    "        if i != 0:\n",
    "            indexes.append(idx)\n",
    "\n",
    "    for i in indexes:\n",
    "        if i in label_idx_list:\n",
    "            wrong_excluded_lable_pred.append(i)  \n",
    "    return wrong_excluded_lable_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test prediction accuracy: 83.56%\n",
      "test prediction error: 16.439999999999998%\n",
      "misclass percentage among excluded class : 2.4001342732460555%\n",
      "misclass percentage among misclassified labels : 8.69829683698297%\n"
     ]
    }
   ],
   "source": [
    "threes_idx = np.where(y_train[:,2] == 1)[0]\n",
    "non_threes_idx = np.where(y_train[:,2] == 0)[0]\n",
    "threes_n_examples = len(threes_idx)\n",
    "\n",
    "predicted = student_m3.predict(X_test)\n",
    "dif = np.argmax(Y_test_new,axis=1) - np.argmax(predicted,axis=1)\n",
    "test_acc = evaluate(predicted, y_test)\n",
    "test_error_rate = np.count_nonzero(dif) / 10000\n",
    "wrong_threes = excluded_label_pred(dif, threes_idx)\n",
    "excluded_lable_miscls_among_label= len(wrong_threes)/threes_n_examples * 100\n",
    "excluded_lable_miscls_among_miscls=len(wrong_threes)/(test_error_rate*10000) * 100\n",
    "\n",
    "print(f'test prediction accuracy: {test_acc * 100}%')\n",
    "print(f'test prediction error: {test_error_rate * 100}%')\n",
    "print(f'misclass percentage among excluded class : {excluded_lable_miscls_among_label}%')\n",
    "print(f'misclass percentage among misclassified labels : {excluded_lable_miscls_among_miscls}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
