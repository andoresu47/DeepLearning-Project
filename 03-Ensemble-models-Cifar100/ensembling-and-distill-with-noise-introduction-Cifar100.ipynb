{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import cifar100\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from CNN import ThreeLayerConvNet\n",
    "from model import myModel\n",
    "from keras.models import Model\n",
    " \n",
    "import timeit, time, pickle, h5py, os, math, copy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_size_for_ensembling(data_set, data_set_logits, test_acc, cls ):\n",
    "    dataset_size= data_set.shape[0]\n",
    "    ensemble_acc = []\n",
    "    best_model_acc = []\n",
    "    class_ensemb_better =[]\n",
    "    class_best_model_better =[]\n",
    "    best_net = np.argmax(test_acc)\n",
    "    for i in range(len(data_set_logits)):\n",
    "        i=i+1\n",
    "        print(('Ensembled models: {0}').format(i))\n",
    "        #best net of the ensemble models\n",
    "        print(('Best model of models: {0}').format(best_net))\n",
    "        ensemble_pred_labels = np.mean(data_set_logits[i], axis=0)\n",
    "        ensemble_cls_pred = np.argmax(ensemble_pred_labels, axis=1)\n",
    "        ensemble_correct = (ensemble_cls_pred == cls.T)\n",
    "        #compute ensemble models accuracy:\n",
    "        ensb_acc = np.sum(ensemble_correct)/dataset_size\n",
    "        ensemble_acc.append(ensb_acc) \n",
    "        print(('Ensembled models accuracy: {0}').format(ensb_acc))\n",
    "        ensemble_incorrect = np.logical_not(ensemble_correct)\n",
    "        \n",
    "        #compute best model's accuracy:\n",
    "        best_net_pred_labels = data_set_logits[i][best_net, :, :]\n",
    "        best_net_cls_pred = np.argmax(best_net_pred_labels, axis=1)\n",
    "        best_net_correct = (best_net_cls_pred == cls.T)\n",
    "        best_model_acc_ = np.sum(best_net_correct)/dataset_size\n",
    "        best_model_acc.append(best_model_acc_)\n",
    "        print(('Single best model accuracy: {0}').format(best_model_acc_))\n",
    "        best_net_incorrect = np.logical_not(best_net_correct)\n",
    "\n",
    "        #ensemble better than best model:\n",
    "        ensemble_better = np.logical_and(best_net_incorrect,\n",
    "                                         ensemble_correct)\n",
    "        ensb_better = ensemble_better.sum()/best_net_incorrect.sum()\n",
    "        class_ensemb_better.append(ensb_better)\n",
    "        print(('Better classified by ensembled and not by best model: {0}%').format(ensb_better * 100))\n",
    "        #best model better than ensemble:\n",
    "        best_net_better = np.logical_and(best_net_correct,\n",
    "                                         ensemble_incorrect)\n",
    "        bm_better =best_net_better.sum()/ensemble_incorrect.sum()\n",
    "        class_best_model_better.append(bm_better)\n",
    "        print(('Better classified by best model and not by ensemble model: {0}%').format(bm_better*100))\n",
    "    \n",
    "    return ensemble_acc, class_ensemb_better, class_best_model_better, best_model_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    # Create a randomized index into the full / combined training-set.\n",
    "    idx = np.random.permutation(images_train.shape[0])\n",
    "\n",
    "    # Split the random index into training- and validation-sets.\n",
    "    idx_train = idx[0:train_size]\n",
    "    idx_validation = idx[train_size:]\n",
    "\n",
    "    # Select the images and labels for the new training-set.\n",
    "    x_train = images_train[idx_train, :]\n",
    "    y_train = labels_train[idx_train, :]\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "def random_batch(x_train, y_train):\n",
    "    # Total number of images in the training-set.\n",
    "    num_images = len(x_train)\n",
    "\n",
    "    # Create a random index into the training-set.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = x_train[idx, :]  # Images.\n",
    "    y_batch = y_train[idx, :]  # Labels.\n",
    "\n",
    "    # Return the batch.\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(images, training):\n",
    "    # Use TensorFlow to loop over all the input images and call\n",
    "    # the function above which takes a single image as input.\n",
    "    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n",
    "\n",
    "    return images\n",
    "\n",
    "def pre_process_image(image, training):\n",
    "    # This function takes a single image as input,\n",
    "    # and a boolean whether to build the training or testing graph.\n",
    "    \n",
    "    if training:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Randomly crop the input image.\n",
    "#         image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n",
    "\n",
    "        # Randomly flip the image horizontally.\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        # Some of these functions may overflow and result in pixel\n",
    "        # values beyond the [0, 1] range. It is unclear from the\n",
    "        # documentation of TensorFlow 0.10.0rc0 whether this is\n",
    "        # intended. A simple solution is to limit the range.\n",
    "\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        image = tf.minimum(image, 1.0)\n",
    "        image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Crop the input image around the centre so it is the same\n",
    "        # size as images that are randomly cropped during training.\n",
    "#         image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "#                                                        target_height=img_size_cropped,\n",
    "#                                                        target_width=img_size_cropped)\n",
    "        pass\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_and_replicate_cifar100_dataset(images_train_, labels_train_, number_of_replicate):\n",
    "    \"\"\"\n",
    "    Takes preprocessed training data: scaled between 0 and 1 for the inputs and one hot representation\n",
    "    for the labels , and returns concatenated arrays of number_of_replicate identical replica in order to perform\n",
    "    data augmentation.\n",
    "    Ex: number_of_replicate = 1 , images_train_.shape = (50000, 32, 32, 3), labels_train_.shape = (50000, 100)\n",
    "    >>> preprocess_and_replicate_cifar100_dataset(images_train_, labels_train_, 1) = concat_inputs, concat_labels\n",
    "    concat_inputs.shape = (100000, 32, 32, 3), concat_labels.shape = (100000, 100)\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs_concat = copy.deepcopy(images_train_)\n",
    "    labels_concat = copy.deepcopy(labels_train_)\n",
    "    \n",
    "    # stacks identical inputs labels arrays\n",
    "    for replicate_step in range(number_of_replicate):\n",
    "        inputs_concat = np.concatenate((inputs_concat, images_train_), axis=0)\n",
    "        labels_concat = np.concatenate((labels_concat, labels_train_), axis=0)\n",
    "    \n",
    "    # to avoid any side effects of concatenating identical arrays, shuffles both of them\n",
    "    shuffle_idx = np.random.permutation(inputs_concat.shape[0])\n",
    "    \n",
    "    return inputs_concat[shuffle_idx], labels_concat[shuffle_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data, create one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images_train, cls_train), (images_test, cls_test) =  cifar100.load_data()\n",
    "images_train = images_train.astype('float32')\n",
    "images_test = images_test.astype('float32')\n",
    "images_train /= 255\n",
    "images_test /= 255\n",
    "labels_train = to_categorical(cls_train, n_classes)\n",
    "labels_test = to_categorical(cls_test, n_classes)\n",
    "n_classes = labels_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will train the models with only 80% of the data:\n",
    "train_size = int(0.8 * (images_train.shape[0]))\n",
    "num_classes, img_size, num_channels = labels_train.shape[1], images_train.shape[1], images_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network: 1\n",
      "Epoch: 1 cost = 4.267  test accuracy: 0.082\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.0823 \n",
      "Epoch: 2 cost = 3.877  test accuracy: 0.118\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.1181 \n",
      "Epoch: 3 cost = 3.672  test accuracy: 0.143\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.1434 \n",
      "Epoch: 4 cost = 3.539  test accuracy: 0.169\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.1685 \n",
      "Epoch: 5 cost = 3.449  test accuracy: 0.179\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.179 \n",
      "Epoch: 6 cost = 3.367  test accuracy: 0.189\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.1892 \n",
      "Epoch: 7 cost = 3.251  test accuracy: 0.211\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2112 \n",
      "Epoch: 8 cost = 3.171  test accuracy: 0.214\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2145 \n",
      "Epoch: 9 cost = 3.093  test accuracy: 0.235\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2352 \n",
      "Epoch: 10 cost = 3.028  test accuracy: 0.241\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.241 \n",
      "Epoch: 11 cost = 2.964  test accuracy: 0.243\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2432 \n",
      "Epoch: 12 cost = 2.910  test accuracy: 0.250\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2501 \n",
      "Epoch: 13 cost = 2.861  test accuracy: 0.253\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2526 \n",
      "Epoch: 14 cost = 2.813  test accuracy: 0.265\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2654 \n",
      "Epoch: 15 cost = 2.769  test accuracy: 0.265\n",
      "Epoch: 16 cost = 2.739  test accuracy: 0.273\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2727 \n",
      "Epoch: 17 cost = 2.690  test accuracy: 0.282\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2822 \n",
      "Epoch: 18 cost = 2.641  test accuracy: 0.281\n",
      "Epoch: 19 cost = 2.598  test accuracy: 0.294\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2935 \n",
      "Epoch: 20 cost = 2.564  test accuracy: 0.288\n",
      "Epoch: 21 cost = 2.521  test accuracy: 0.295\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.2946 \n",
      "Epoch: 22 cost = 2.504  test accuracy: 0.294\n",
      "Epoch: 23 cost = 2.468  test accuracy: 0.302\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.3023 \n",
      "Epoch: 24 cost = 2.443  test accuracy: 0.295\n",
      "Epoch: 25 cost = 2.409  test accuracy: 0.305\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.3053 \n",
      "Epoch: 26 cost = 2.389  test accuracy: 0.301\n",
      "Epoch: 27 cost = 2.349  test accuracy: 0.309\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.3094 \n",
      "Epoch: 28 cost = 2.316  test accuracy: 0.306\n",
      "Epoch: 29 cost = 2.311  test accuracy: 0.315\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network10.3153 \n",
      "Epoch: 30 cost = 2.273  test accuracy: 0.310\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test accuracy:\n",
      "Time usage: 0:42:12\n",
      "Neural network: 2\n",
      "Epoch: 1 cost = 4.196  test accuracy: 0.101\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.1012 \n",
      "Epoch: 2 cost = 3.755  test accuracy: 0.145\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.1446 \n",
      "Epoch: 3 cost = 3.544  test accuracy: 0.179\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.1794 \n",
      "Epoch: 4 cost = 3.380  test accuracy: 0.195\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.195 \n",
      "Epoch: 5 cost = 3.257  test accuracy: 0.217\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.217 \n",
      "Epoch: 6 cost = 3.155  test accuracy: 0.226\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2256 \n",
      "Epoch: 7 cost = 3.065  test accuracy: 0.238\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2385 \n",
      "Epoch: 8 cost = 2.991  test accuracy: 0.244\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2445 \n",
      "Epoch: 9 cost = 2.922  test accuracy: 0.251\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2509 \n",
      "Epoch: 10 cost = 2.866  test accuracy: 0.262\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2625 \n",
      "Epoch: 11 cost = 2.823  test accuracy: 0.265\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2649 \n",
      "Epoch: 12 cost = 2.770  test accuracy: 0.275\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2749 \n",
      "Epoch: 13 cost = 2.713  test accuracy: 0.278\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2778 \n",
      "Epoch: 14 cost = 2.671  test accuracy: 0.268\n",
      "Epoch: 15 cost = 2.636  test accuracy: 0.285\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2853 \n",
      "Epoch: 16 cost = 2.588  test accuracy: 0.285\n",
      "Epoch: 17 cost = 2.564  test accuracy: 0.288\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.288 \n",
      "Epoch: 18 cost = 2.536  test accuracy: 0.293\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.2928 \n",
      "Epoch: 19 cost = 2.497  test accuracy: 0.289\n",
      "Epoch: 20 cost = 2.470  test accuracy: 0.293\n",
      "Epoch: 21 cost = 2.437  test accuracy: 0.291\n",
      "Epoch: 22 cost = 2.416  test accuracy: 0.288\n",
      "Epoch: 23 cost = 2.381  test accuracy: 0.301\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.3011 \n",
      "Epoch: 24 cost = 2.351  test accuracy: 0.301\n",
      "Epoch: 25 cost = 2.329  test accuracy: 0.301\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.3012 \n",
      "Epoch: 26 cost = 2.313  test accuracy: 0.299\n",
      "Epoch: 27 cost = 2.290  test accuracy: 0.311\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.311 \n",
      "Epoch: 28 cost = 2.255  test accuracy: 0.300\n",
      "Epoch: 29 cost = 2.259  test accuracy: 0.304\n",
      "Epoch: 30 cost = 2.223  test accuracy: 0.313\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network20.3129 \n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test accuracy:\n",
      "Time usage: 0:42:54\n",
      "Neural network: 3\n",
      "Epoch: 1 cost = 4.290  test accuracy: 0.095\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.0953 \n",
      "Epoch: 2 cost = 3.814  test accuracy: 0.135\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.1352 \n",
      "Epoch: 3 cost = 3.596  test accuracy: 0.158\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.1576 \n",
      "Epoch: 4 cost = 3.464  test accuracy: 0.179\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.1795 \n",
      "Epoch: 5 cost = 3.356  test accuracy: 0.181\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.1807 \n",
      "Epoch: 6 cost = 3.258  test accuracy: 0.206\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2063 \n",
      "Epoch: 7 cost = 3.195  test accuracy: 0.217\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2173 \n",
      "Epoch: 8 cost = 3.124  test accuracy: 0.234\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.234 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 cost = 3.047  test accuracy: 0.242\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2416 \n",
      "Epoch: 10 cost = 3.001  test accuracy: 0.246\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.246 \n",
      "Epoch: 11 cost = 2.953  test accuracy: 0.252\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2515 \n",
      "Epoch: 12 cost = 2.918  test accuracy: 0.256\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2559 \n",
      "Epoch: 13 cost = 2.880  test accuracy: 0.259\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2592 \n",
      "Epoch: 14 cost = 2.830  test accuracy: 0.265\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2646 \n",
      "Epoch: 15 cost = 2.800  test accuracy: 0.269\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2693 \n",
      "Epoch: 16 cost = 2.753  test accuracy: 0.277\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.277 \n",
      "Epoch: 17 cost = 2.723  test accuracy: 0.284\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2842 \n",
      "Epoch: 18 cost = 2.683  test accuracy: 0.277\n",
      "Epoch: 19 cost = 2.656  test accuracy: 0.285\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2849 \n",
      "Epoch: 20 cost = 2.611  test accuracy: 0.284\n",
      "Epoch: 21 cost = 2.595  test accuracy: 0.284\n",
      "Epoch: 22 cost = 2.553  test accuracy: 0.291\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2906 \n",
      "Epoch: 23 cost = 2.521  test accuracy: 0.291\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.2912 \n",
      "Epoch: 24 cost = 2.507  test accuracy: 0.304\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.3044 \n",
      "Epoch: 25 cost = 2.463  test accuracy: 0.299\n",
      "Epoch: 26 cost = 2.453  test accuracy: 0.301\n",
      "Epoch: 27 cost = 2.443  test accuracy: 0.308\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.3084 \n",
      "Epoch: 28 cost = 2.405  test accuracy: 0.307\n",
      "Epoch: 29 cost = 2.387  test accuracy: 0.311\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.3106 \n",
      "Epoch: 30 cost = 2.341  test accuracy: 0.317\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network30.3167 \n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test accuracy:\n",
      "Time usage: 0:42:54\n",
      "Neural network: 4\n",
      "Epoch: 1 cost = 4.285  test accuracy: 0.089\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.0885 \n",
      "Epoch: 2 cost = 3.808  test accuracy: 0.131\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.1307 \n",
      "Epoch: 3 cost = 3.603  test accuracy: 0.164\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.1636 \n",
      "Epoch: 4 cost = 3.464  test accuracy: 0.178\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.1779 \n",
      "Epoch: 5 cost = 3.337  test accuracy: 0.191\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.1909 \n",
      "Epoch: 6 cost = 3.259  test accuracy: 0.212\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2118 \n",
      "Epoch: 7 cost = 3.169  test accuracy: 0.209\n",
      "Epoch: 8 cost = 3.094  test accuracy: 0.231\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2306 \n",
      "Epoch: 9 cost = 3.035  test accuracy: 0.237\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2366 \n",
      "Epoch: 10 cost = 2.993  test accuracy: 0.249\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.249 \n",
      "Epoch: 11 cost = 2.931  test accuracy: 0.249\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2492 \n",
      "Epoch: 12 cost = 2.863  test accuracy: 0.255\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.255 \n",
      "Epoch: 13 cost = 2.819  test accuracy: 0.262\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2625 \n",
      "Epoch: 14 cost = 2.774  test accuracy: 0.268\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2682 \n",
      "Epoch: 15 cost = 2.738  test accuracy: 0.280\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2796 \n",
      "Epoch: 16 cost = 2.690  test accuracy: 0.290\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2904 \n",
      "Epoch: 17 cost = 2.658  test accuracy: 0.288\n",
      "Epoch: 18 cost = 2.617  test accuracy: 0.291\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2913 \n",
      "Epoch: 19 cost = 2.593  test accuracy: 0.288\n",
      "Epoch: 20 cost = 2.557  test accuracy: 0.288\n",
      "Epoch: 21 cost = 2.533  test accuracy: 0.294\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2942 \n",
      "Epoch: 22 cost = 2.493  test accuracy: 0.296\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.2956 \n",
      "Epoch: 23 cost = 2.483  test accuracy: 0.292\n",
      "Epoch: 24 cost = 2.431  test accuracy: 0.293\n",
      "Epoch: 25 cost = 2.418  test accuracy: 0.299\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.299 \n",
      "Epoch: 26 cost = 2.400  test accuracy: 0.303\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.3034 \n",
      "Epoch: 27 cost = 2.372  test accuracy: 0.304\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.304 \n",
      "Epoch: 28 cost = 2.349  test accuracy: 0.310\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.3101 \n",
      "Epoch: 29 cost = 2.331  test accuracy: 0.308\n",
      "Epoch: 30 cost = 2.308  test accuracy: 0.311\n",
      "Model Checkpointed to checkpoints_/cifar100/preprocessed/teacher-model-i-ensembling_network40.3109 \n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test accuracy:\n",
      "Time usage: 0:43:30\n",
      "Neural network: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-359f2bdb5f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_dir = 'logs/cifar100/'\n",
    "train_batch_size = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "num_networks = 10\n",
    "epochs = 30\n",
    "# tf.reset_default_graph()\n",
    "for i in range(num_networks):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        checkpoint_file=\"teacher-model-i-ensembling_network\"+str(i+1)\n",
    "        checkpoint_dir = 'checkpoints_/cifar100/preprocessed/'\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        #x has shape 32x32x3\n",
    "        x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "        #y has shape nx100\n",
    "        y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "        #32x32x3\n",
    "#         input_data=x\n",
    "        input_data = pre_process(images=x, training=True)\n",
    "        num_input_channels=3 #3 color layers\n",
    "        num_filters=32 #by choice\n",
    "        filter_shape=[5, 5] #each filter dimension\n",
    "        pool_shape=[2, 2] # downsampling operation\n",
    "        name='layer1'\n",
    "        #the filter input shape = 5x5x3x32\n",
    "        conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "        #5x5x3 = 75 weights per filtes => 75x32 filters = 2400 weights\n",
    "        weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "        #32 bias\n",
    "        bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "        #32x32x32\n",
    "        layer1 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        layer1 += bias\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "        ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "        strides = [1, 2, 2, 1]\n",
    "        #16x16x32\n",
    "        layer1 = tf.nn.max_pool(layer1, ksize=ksize, strides=strides, padding='SAME')\n",
    "       #==========================================================================================\n",
    "        #second Convolutional Layer:\n",
    "        #==========================================================================================\n",
    "        #16x16x64\n",
    "        input_data= layer1\n",
    "        num_input_channels=32 # number of filtes of previous layer\n",
    "        num_filters=64 #by choice\n",
    "        filter_shape=[5, 5]\n",
    "        pool_shape=[2, 2]\n",
    "        name='layer2'\n",
    "        #\n",
    "        conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "        #5x5x32 = 800 weights per filter =>  800x64 = 51200 weights\n",
    "        weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "        #64 bias\n",
    "        bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "        #16x16x64\n",
    "        layer2 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        layer2 += bias\n",
    "        layer2 = tf.nn.relu(layer2)\n",
    "        ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "        strides = [1, 2, 2, 1]\n",
    "        #8x8x64\n",
    "        layer2 = tf.nn.max_pool(layer2, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "        #==========================================================================================\n",
    "        #THIRD CONV LAYER\n",
    "        #==========================================================================================\n",
    "        #8x8x64\n",
    "        input_data= layer2\n",
    "        num_input_channels=64 # number of filtes of previous layer\n",
    "        num_filters=128 #by choice\n",
    "        filter_shape=[5, 5]\n",
    "        pool_shape=[2, 2]\n",
    "        name='layer3'\n",
    "        conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "        #5x5x64= 1600 weights per filter => 1600x128 = 204800\n",
    "        weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "        #128\n",
    "        bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "        #8x8x64\n",
    "        layer3 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        layer3 += bias\n",
    "        layer3 = tf.nn.relu(layer3)\n",
    "        ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "        strides = [1, 2, 2, 1]\n",
    "        #4x4x128\n",
    "        layer3 = tf.nn.max_pool(layer3, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "        #==========================================================================================\n",
    "        flattened = tf.reshape(layer3, [-1, 4 * 4 * 128])\n",
    "        #1000 hidden nodes.\n",
    "        wd1 = tf.Variable(tf.truncated_normal([4 * 4 * 128, 100], stddev=0.03), name='wd1')\n",
    "        bd1 = tf.Variable(tf.truncated_normal([100], stddev=0.01), name='bd1')\n",
    "        \n",
    "        dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "        dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "        \n",
    "#         drop_out = tf.nn.dropout(dense_layer1, 0.5)\n",
    "        # another layer with softmax activations\n",
    "        wd2 = tf.Variable(tf.truncated_normal([100, 100], stddev=0.03), name='wd2')\n",
    "        bd2 = tf.Variable(tf.truncated_normal([100], stddev=0.01), name='bd2')\n",
    "        dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "        y_ = tf.nn.softmax(dense_layer2)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y_true))\n",
    "        optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "        # define an accuracy assessment operation\n",
    "        correct_prediction = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        # setup the initialisation operator\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        # setup recording variables\n",
    "        # add a summary to store the accuracy\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('project-test'+checkpoint_file+str(i))\n",
    "        saver = tf.train.Saver()\n",
    "        #==========================================================================================\n",
    "        #Start training model:\n",
    "        #==========================================================================================\n",
    "        print(\"Neural network: {0}\".format(i+1)) \n",
    "        num_iterations = 20000\n",
    "        max_acc = 0.0\n",
    "        start_time = time.time()\n",
    "        x_train, y_train = random_training_set()\n",
    "        x_train, y_train = preprocess_and_replicate_cifar100_dataset(x_train, y_train, 1)\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(images_train.shape[0] / batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = random_batch(x_train,y_train)\n",
    "                _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: images_test, y_true: labels_test})\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \" test accuracy: {:.3f}\".format(test_acc))\n",
    "            summary = sess.run(merged, feed_dict={x: images_test, y_true: labels_test})\n",
    "#         print((accuracy))\n",
    "            if test_acc > max_acc:\n",
    "                save_path = saver.save(sess, checkpoint_path+str(test_acc))\n",
    "                print(\"Model Checkpointed to %s \" % (save_path))\n",
    "                max_acc = test_acc    \n",
    "        print(\"\\nTraining complete!\")\n",
    "        writer.add_graph(sess.graph)\n",
    "        print(\"\\nTest accuracy:\")\n",
    "        # Ending time.\n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs/cifar100/'\n",
    "train_batch_size = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "num_networks = 10\n",
    "epochs = 30\n",
    "i=1\n",
    "with tf.Session() as sess:\n",
    "    checkpoint_file=\"teacher-model-i-ensembling_network\"+str(i+1)\n",
    "    checkpoint_dir = 'checkpoints_/cifar100/preprocessed/'\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "    #x has shape 32x32x3\n",
    "    x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "    #y has shape nx100\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "    #32x32x3\n",
    "#         input_data=x\n",
    "    input_data = pre_process(images=x, training=True)\n",
    "    num_input_channels=3 #3 color layers\n",
    "    num_filters=32 #by choice\n",
    "    filter_shape=[5, 5] #each filter dimension\n",
    "    pool_shape=[2, 2] # downsampling operation\n",
    "    name='layer1'\n",
    "    #the filter input shape = 5x5x3x32\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "    #5x5x3 = 75 weights per filtes => 75x32 filters = 2400 weights\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "    #32 bias\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "    #32x32x32\n",
    "    layer1 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "    layer1 += bias\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    #16x16x32\n",
    "    layer1 = tf.nn.max_pool(layer1, ksize=ksize, strides=strides, padding='SAME')\n",
    "   #==========================================================================================\n",
    "    #second Convolutional Layer:\n",
    "    #==========================================================================================\n",
    "    #16x16x64\n",
    "    input_data= layer1\n",
    "    num_input_channels=32 # number of filtes of previous layer\n",
    "    num_filters=64 #by choice\n",
    "    filter_shape=[5, 5]\n",
    "    pool_shape=[2, 2]\n",
    "    name='layer2'\n",
    "    #\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "    #5x5x32 = 800 weights per filter =>  800x64 = 51200 weights\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "    #64 bias\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "    #16x16x64\n",
    "    layer2 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "    layer2 += bias\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    #8x8x64\n",
    "    layer2 = tf.nn.max_pool(layer2, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "    #==========================================================================================\n",
    "    #THIRD CONV LAYER\n",
    "    #==========================================================================================\n",
    "    #8x8x64\n",
    "    input_data= layer2\n",
    "    num_input_channels=64 # number of filtes of previous layer\n",
    "    num_filters=128 #by choice\n",
    "    filter_shape=[5, 5]\n",
    "    pool_shape=[2, 2]\n",
    "    name='layer3'\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "    #5x5x64= 1600 weights per filter => 1600x128 = 204800\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "    #128\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "    #8x8x64\n",
    "    layer3 = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "    layer3 += bias\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    #4x4x128\n",
    "    layer3 = tf.nn.max_pool(layer3, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "    #==========================================================================================\n",
    "    flattened = tf.reshape(layer3, [-1, 4 * 4 * 128])\n",
    "    #1000 hidden nodes.\n",
    "    wd1 = tf.Variable(tf.truncated_normal([4 * 4 * 128, 100], stddev=0.03), name='wd1')\n",
    "    bd1 = tf.Variable(tf.truncated_normal([100], stddev=0.01), name='bd1')\n",
    "\n",
    "    dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "    dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "#         drop_out = tf.nn.dropout(dense_layer1, 0.5)\n",
    "    # another layer with softmax activations\n",
    "    wd2 = tf.Variable(tf.truncated_normal([100, 100], stddev=0.03), name='wd2')\n",
    "    bd2 = tf.Variable(tf.truncated_normal([100], stddev=0.01), name='bd2')\n",
    "    dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "    y_ = tf.nn.softmax(dense_layer2)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y_true))\n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # setup recording variables\n",
    "    # add a summary to store the accuracy\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('project-test'+checkpoint_file+str(i))\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "checkpoint_dir = 'checkpoints_/cifar100/preprocessed/'\n",
    "onlyfiles = [f for f in listdir(checkpoint_dir) if isfile(join(checkpoint_dir, f))]\n",
    "test_decomp_str = sorted(\n",
    "    [\n",
    "        (foldername.split('.')[0], foldername.split('.')[1])\n",
    "        for foldername in onlyfiles\n",
    "        if len(foldername.split('.')) > 2 and\n",
    "        foldername.split('.')[-1] == 'meta'\n",
    "    ],\n",
    "    reverse=True,\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "top_15 = test_decomp_str[:15]\n",
    "top_15_filenames = [filerootname + '.' + acc_ for filerootname, acc_ in top_15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teacher-model-i-ensembling_network30.3167',\n",
       " 'teacher-model-i-ensembling_network10.3153',\n",
       " 'teacher-model-i-ensembling_network20.3129',\n",
       " 'teacher-model-i-ensembling_network20.311',\n",
       " 'teacher-model-i-ensembling_network40.3109',\n",
       " 'teacher-model-i-ensembling_network30.3106',\n",
       " 'teacher-model-i-ensembling_network40.3101',\n",
       " 'teacher-model-i-ensembling_network10.3094',\n",
       " 'teacher-model-i-ensembling_network30.3084',\n",
       " 'teacher-model-i-ensembling_network10.3053',\n",
       " 'teacher-model-i-ensembling_network30.3044',\n",
       " 'teacher-model-i-ensembling_network40.304',\n",
       " 'teacher-model-i-ensembling_network40.3034',\n",
       " 'teacher-model-i-ensembling_network10.3023',\n",
       " 'teacher-model-i-ensembling_network20.3012']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_15_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciate TensorFlow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get logits, sofmaxs, verify training and test accuracy of the best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ensemble models:15\n",
      "Network: 15, Test-Set: 0.3275, Train-Set: 0.4097\n",
      "Network: 15, Test-Set: 0.3263, Train-Set: 0.4252\n",
      "Network: 15, Test-Set: 0.3236, Train-Set: 0.4367\n",
      "Network: 15, Test-Set: 0.3148, Train-Set: 0.4210\n",
      "Network: 15, Test-Set: 0.3274, Train-Set: 0.4256\n",
      "Network: 15, Test-Set: 0.3308, Train-Set: 0.4068\n",
      "Network: 15, Test-Set: 0.3287, Train-Set: 0.4202\n",
      "Network: 15, Test-Set: 0.3239, Train-Set: 0.4097\n",
      "Network: 15, Test-Set: 0.3249, Train-Set: 0.3964\n",
      "Network: 15, Test-Set: 0.3222, Train-Set: 0.4044\n",
      "Network: 15, Test-Set: 0.3225, Train-Set: 0.3886\n",
      "Network: 15, Test-Set: 0.3238, Train-Set: 0.4069\n",
      "Network: 15, Test-Set: 0.3189, Train-Set: 0.4066\n",
      "Network: 15, Test-Set: 0.3199, Train-Set: 0.3936\n",
      "Network: 15, Test-Set: 0.3145, Train-Set: 0.4188\n"
     ]
    }
   ],
   "source": [
    "i=15\n",
    "num_training_images = images_train.shape[0]\n",
    "n_classes= labels_train.shape[1]\n",
    "print(('Number of ensemble models:{0}').format(i))\n",
    "# Reload the variables into the TensorFlow graph.\n",
    "checkpoint_dir = 'checkpoints_/cifar100/preprocessed/'\n",
    "num_training_images = images_train.shape[0]\n",
    "files_list = top_15_filenames\n",
    "num_networks = len(files_list)\n",
    "batch_size=1000\n",
    "test_accuracies = []\n",
    "training_accuracies = []\n",
    "concatenated_logits_training = np.zeros((num_networks, num_training_images, n_classes))\n",
    "concatenated_softmax_training = np.zeros((num_networks, num_training_images, n_classes))\n",
    "\n",
    "logits_test = []\n",
    "softmax_test = []\n",
    "for idx, file in enumerate(files_list):\n",
    "#             checkpoint_dir = 'checkpoints_/cifar10/new/'\n",
    "#             saver = tf.train.import_meta_graph(checkpoint_dir + file+ '.meta')\n",
    "    saver.restore(sess=session, save_path= checkpoint_dir + file)\n",
    "    test_acc = session.run(accuracy, feed_dict={x: images_test, y_true: labels_test})\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    logits_test_ = session.run(dense_layer2, feed_dict={x: images_test})\n",
    "    logits_test.append(logits_test_)\n",
    "\n",
    "    sotfmax_test_ = session.run(y_, feed_dict={x: images_test})\n",
    "    softmax_test.append(sotfmax_test_)\n",
    "\n",
    "    #Training Logits\n",
    "    batch_size=1000\n",
    "    logits_training = []\n",
    "    for batch_idx in range(images_train.shape[0] // batch_size + 1):\n",
    "        #print(\"new batch\")\n",
    "        X_batch = images_train[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        batch_logit_train = session.run(dense_layer2, feed_dict={x: X_batch})\n",
    "        logits_training.append(batch_logit_train)\n",
    "    logits_training = np.array(logits_training)\n",
    "    concatenated_logits_training_ = logits_training[0]\n",
    "    for batch_prediction in logits_training[1:]:\n",
    "        concatenated_logits_training_ = np.concatenate((concatenated_logits_training_, batch_prediction), axis=0)\n",
    "    concatenated_logits_training[idx] = concatenated_logits_training_\n",
    "\n",
    "    #Training Softmax\n",
    "    softmax_training = []\n",
    "    for batch_idx in range(images_train.shape[0] // batch_size + 1):\n",
    "        X_batch = images_train[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        batch_softmax_train = session.run(y_, feed_dict={x: X_batch})\n",
    "        softmax_training.append(batch_softmax_train)\n",
    "    softmax_training = np.array(softmax_training)\n",
    "    concatenated_softmax_training_ = softmax_training[0]\n",
    "    for batch_prediction in softmax_training[1:]:\n",
    "        concatenated_softmax_training_ = np.concatenate((concatenated_softmax_training_, batch_prediction), axis=0)\n",
    "    concatenated_softmax_training[idx] = concatenated_softmax_training_\n",
    "\n",
    "    #Training Softmax\n",
    "    train_acc = np.mean( np.argmax(concatenated_softmax_training_, axis=1) == cls_train.T[0])\n",
    "    training_accuracies.append(train_acc)\n",
    "\n",
    "    msg = \"Network: {0}, Test-Set: {1:.4f}, Train-Set: {2:.4f}\"\n",
    "    print(msg.format(i, test_acc, train_acc))\n",
    "# ensembels_logits_train[i] = concatenated_logits_training\n",
    "# ensembels_softmax_train[i] = concatenated_softmax_training\n",
    "# ensembels_test_acc[i] = test_accuracies\n",
    "# ensembels_logit_test[i] = np.array(logits_test)\n",
    "# ensembels_models_test[i] = np.array(softmax_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format data before ensembling.\n",
    "training_im_len = images_train.shape[0]\n",
    "n_models = len(top_15_filenames)\n",
    "array = np.argsort(test_accuracies)\n",
    "reversed_arr = array[::-1]\n",
    "test_accuracies = -np.sort(-np.asanyarray(test_accuracies))\n",
    "\n",
    "concatenated_logits_training_ = np.zeros((len(top_15_filenames), images_train.shape[0], num_classes ))\n",
    "concatenated_softmax_training_ = np.zeros((len(top_15_filenames), images_train.shape[0], num_classes ))\n",
    "logits_test_ = np.zeros((n_models, training_im_len*2, num_classes ))\n",
    "softmax_test_ = np.zeros((n_models, training_im_len*2, num_classes ))\n",
    "for idx, i in enumerate(reversed_arr):\n",
    "    concatenated_logits_training_[idx] = concatenated_logits_training[i]\n",
    "    concatenated_softmax_training_[idx] = concatenated_softmax_training[i]\n",
    "    logits_test_[idx] = logits_test[i]\n",
    "    softmax_test_[idx]= softmax_test[i]\n",
    "#     test_accuracies_[idx+1] = test_accuracies[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the ensemble of models in deschending order.\n",
    "ensemble_logits_training = {}\n",
    "ensemble_softmax_training = {}\n",
    "ensemble_logits_test = {}\n",
    "ensemble_softmax_test = {}\n",
    "for i in range(len(test_accuracies)):\n",
    "    ensemble_logits_training[i+1] = concatenated_logits_training_[:i+1]\n",
    "    ensemble_softmax_training[i+1] = concatenated_softmax_training_[:i+1]\n",
    "    ensemble_logits_test[i+1] = logits_test_[:i+1]\n",
    "    ensemble_softmax_test[i+1] = softmax_test_[:i+1]\n",
    "\n",
    "logits_test =  np.array(logits_test_)\n",
    "softmax_test = np.array(softmax_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions, the accuracy should be the same whether is computed with the logits or softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled models: 1\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.40684\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 0.0%\n",
      "Better classified by best model and not by ensemble model: 0.0%\n",
      "Ensembled models: 2\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.48706\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 21.090430912401377%\n",
      "Better classified by best model and not by ensemble model: 8.749561352204935%\n",
      "Ensembled models: 3\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4897\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 19.6540562411491%\n",
      "Better classified by best model and not by ensemble model: 6.6078777189888305%\n",
      "Ensembled models: 4\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.50478\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 24.07444871535505%\n",
      "Better classified by best model and not by ensemble model: 9.058600218084893%\n",
      "Ensembled models: 5\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.52974\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 27.631667678198124%\n",
      "Better classified by best model and not by ensemble model: 8.71858121039425%\n",
      "Ensembled models: 6\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.52504\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 26.03681974509407%\n",
      "Better classified by best model and not by ensemble model: 7.6301162203132895%\n",
      "Ensembled models: 7\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.52982\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 27.44959201564502%\n",
      "Better classified by best model and not by ensemble model: 8.473350631672977%\n",
      "Ensembled models: 8\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.5309\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 28.279047811720275%\n",
      "Better classified by best model and not by ensemble model: 9.311447452568748%\n",
      "Ensembled models: 9\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.54608\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 30.48081462000135%\n",
      "Better classified by best model and not by ensemble model: 9.15579837856891%\n",
      "Ensembled models: 10\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.5425\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 29.422078359970328%\n",
      "Better classified by best model and not by ensemble model: 8.49398907103825%\n",
      "Ensembled models: 11\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.54366\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 29.937959403870796%\n",
      "Better classified by best model and not by ensemble model: 8.931936713853705%\n",
      "Ensembled models: 12\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.54172\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 30.12340683795266%\n",
      "Better classified by best model and not by ensemble model: 9.557475778999738%\n",
      "Ensembled models: 13\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.54102\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 30.393148560253557%\n",
      "Better classified by best model and not by ensemble model: 10.04401063227156%\n",
      "Ensembled models: 14\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.54722\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 31.263065614673952%\n",
      "Better classified by best model and not by ensemble model: 9.951852997040506%\n",
      "Ensembled models: 15\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.5505\n",
      "Single best model accuracy: 0.40684\n",
      "Better classified by ensembled and not by best model: 31.82277968844831%\n",
      "Better classified by best model and not by ensemble model: 10.03337041156841%\n"
     ]
    }
   ],
   "source": [
    "#Training data.\n",
    "(ensemble_acc_train,\n",
    " class_ensemb_better_train, \n",
    " class_best_model_better_train,\n",
    " best_model_acc_train) = analize_size_for_ensembling(images_train,\n",
    "                                                     ensemble_logits_training,\n",
    "                                                     test_accuracies,\n",
    "                                                     cls_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembled models: 1\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.3308\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 0.0%\n",
      "Better classified by best model and not by ensemble model: 0.0%\n",
      "Ensembled models: 2\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.3725\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 12.283323371189478%\n",
      "Better classified by best model and not by ensemble model: 6.454183266932271%\n",
      "Ensembled models: 3\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.3727\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 11.147638971906753%\n",
      "Better classified by best model and not by ensemble model: 5.2128168340506935%\n",
      "Ensembled models: 4\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.3833\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 14.465032875074716%\n",
      "Better classified by best model and not by ensemble model: 7.183395492135561%\n",
      "Ensembled models: 5\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4013\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 16.706515242080094%\n",
      "Better classified by best model and not by ensemble model: 6.8982796058125935%\n",
      "Ensembled models: 6\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4017\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 15.989240884638376%\n",
      "Better classified by best model and not by ensemble model: 6.033762326592011%\n",
      "Ensembled models: 7\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.407\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 17.1249252839211%\n",
      "Better classified by best model and not by ensemble model: 6.475548060708262%\n",
      "Ensembled models: 8\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4051\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 17.468619246861923%\n",
      "Better classified by best model and not by ensemble model: 7.160867372667676%\n",
      "Ensembled models: 9\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4147\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 18.873281530185295%\n",
      "Better classified by best model and not by ensemble model: 7.244148300017085%\n",
      "Ensembled models: 10\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4158\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 18.469814704124328%\n",
      "Better classified by best model and not by ensemble model: 6.607326258130778%\n",
      "Ensembled models: 11\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4175\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 18.962940824865512%\n",
      "Better classified by best model and not by ensemble model: 6.901287553648068%\n",
      "Ensembled models: 12\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4158\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 19.157202630005976%\n",
      "Better classified by best model and not by ensemble model: 7.394727832933927%\n",
      "Ensembled models: 13\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4143\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 19.276748356246266%\n",
      "Better classified by best model and not by ensemble model: 7.768482158101417%\n",
      "Ensembled models: 14\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4193\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 20.068738792588164%\n",
      "Better classified by best model and not by ensemble model: 7.88703289133804%\n",
      "Ensembled models: 15\n",
      "Best model of models: 0\n",
      "Ensembled models accuracy: 0.4207\n",
      "Single best model accuracy: 0.3308\n",
      "Better classified by ensembled and not by best model: 20.442319187089062%\n",
      "Better classified by best model and not by ensemble model: 8.09597790436734%\n"
     ]
    }
   ],
   "source": [
    "#Test data.\n",
    "(ensemble_acc_softmx,\n",
    " class_ensemb_better_test_softmx, \n",
    " class_best_model_better_test_softmx,\n",
    " best_model_acc_test_softmx) = analize_size_for_ensembling(images_test,\n",
    "                                                        ensemble_softmax_test,\n",
    "                                                        test_accuracies,\n",
    "                                                        cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_net = np.argmax(test_acc)\n",
    "test_accuracies[best_net]\n",
    "ensemb_models = [i+1 for i in range(15)]\n",
    "fig = plt.figure()\n",
    "sns.set()\n",
    "# plt.scatter(ensemb_models, ensemble_acc_train_softmx, alpha=0.8, label='3-layer CNN + Shufflenet')\n",
    "plt.plot(ensemb_models[1:], ensemble_acc_softmx[1:] ,c='blue',marker='X',markersize=12, linewidth=2, linestyle='dashed')\n",
    "plt.plot(best_net, test_accuracies[best_net], c = \"red\" ,marker='X',markersize=12)\n",
    "plt.xlabel(\"Number of snapshots in ensemble\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.legend(loc='best')\n",
    "# plt.xlim(1, 15)\n",
    "# fig.savefig('noisy_ensemble_cifar100.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save logits for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('best_ensemble_logits_new_cs', ensemble_logits_training[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling the ensemble models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_train_logits = np.load(\n",
    "    'best_ensemble_logits_new_cs.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train with small model\n",
    "data = {'X_train': images_train.transpose(0,3,1,2).copy(), 'y_train': np.argmax(labels_train,axis=1),\n",
    "        'X_val': images_test.transpose(0,3,1,2).copy(), 'y_val': np.argmax(labels_test,axis=1),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * (images_train.shape[0]))\n",
    "num_classes, img_size, num_channels = labels_train.shape[1], images_train.shape[1], images_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftMax(s):\n",
    "    # minus max to avoid large s case\n",
    "    p = np.exp(s-np.expand_dims(np.max(s,axis=1),axis=1))/\\\n",
    "    np.expand_dims(np.exp(s-np.expand_dims(np.max(s,axis=1),axis=1)).sum(axis=1),axis=1)  # matrix of size NxK\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \n",
    "[25,30,35,40,45,50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 500) loss: 592.572072\n",
      "(Epoch 0 / 1) train acc: 0.009960; val_acc: 0.010000\n",
      "(Iteration 101 / 500) loss: 589.732794\n",
      "(Iteration 201 / 500) loss: 589.400423\n",
      "(Iteration 301 / 500) loss: 589.428814\n",
      "(Iteration 401 / 500) loss: 589.116374\n",
      "(Epoch 1 / 1) train acc: 0.253460; val_acc: 0.231000\n",
      "Execution time:  719.8525304794312\n",
      "Test accuracy: 0.231\n",
      "(Iteration 1 / 500) loss: 668.495549\n",
      "(Epoch 0 / 1) train acc: 0.010480; val_acc: 0.010400\n",
      "(Iteration 101 / 500) loss: 666.397427\n",
      "(Iteration 201 / 500) loss: 665.934375\n",
      "(Iteration 301 / 500) loss: 665.588567\n",
      "(Iteration 401 / 500) loss: 665.313779\n",
      "(Epoch 1 / 1) train acc: 0.260460; val_acc: 0.237600\n",
      "Execution time:  717.1030745506287\n",
      "Test accuracy: 0.2376\n",
      "(Iteration 1 / 500) loss: 749.110047\n",
      "(Epoch 0 / 1) train acc: 0.010400; val_acc: 0.010800\n",
      "(Iteration 101 / 500) loss: 746.421123\n",
      "(Iteration 201 / 500) loss: 746.454592\n",
      "(Iteration 301 / 500) loss: 745.967402\n",
      "(Iteration 401 / 500) loss: 745.780998\n",
      "(Epoch 1 / 1) train acc: 0.268340; val_acc: 0.248700\n",
      "Execution time:  718.3612878322601\n",
      "Test accuracy: 0.2487\n",
      "(Iteration 1 / 500) loss: 834.391102\n",
      "(Epoch 0 / 1) train acc: 0.010020; val_acc: 0.010000\n",
      "(Iteration 101 / 500) loss: 831.748942\n",
      "(Iteration 201 / 500) loss: 831.271617\n",
      "(Iteration 301 / 500) loss: 831.147766\n",
      "(Iteration 401 / 500) loss: 830.438438\n",
      "(Epoch 1 / 1) train acc: 0.267540; val_acc: 0.242800\n",
      "Execution time:  716.3138289451599\n",
      "Test accuracy: 0.2428\n",
      "(Iteration 1 / 500) loss: 924.217534\n",
      "(Epoch 0 / 1) train acc: 0.010840; val_acc: 0.010800\n",
      "(Iteration 101 / 500) loss: 922.394390\n",
      "(Iteration 201 / 500) loss: 921.278845\n",
      "(Iteration 301 / 500) loss: 921.252343\n",
      "(Iteration 401 / 500) loss: 921.005420\n",
      "(Epoch 1 / 1) train acc: 0.242740; val_acc: 0.227800\n",
      "Execution time:  717.2763357162476\n",
      "Test accuracy: 0.2278\n"
     ]
    }
   ],
   "source": [
    "temp = [t+1 for t in range(15 , 20)]\n",
    "results = {}\n",
    "for t in temp:\n",
    "    num_networks=15\n",
    "    temp=t\n",
    "    models_s_15 = teacher_train_logits/temp\n",
    "    softmax_ensemb_models_15 = np.zeros_like(models_s_15)\n",
    "    for i in range(num_networks):\n",
    "           softmax_ensemb_models_15[i] = SoftMax(models_s_15[i])\n",
    "    average_softmax_15 = np.mean(softmax_ensemb_models_15, axis=0)\n",
    "    \n",
    "    net_15 = ThreeLayerConvNet(input_dim=(3, 32, 32),num_classes=100,num_filters=16,filter_size=5,hidden_dim=512,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "    small_model_15 = myModel(net_15, data,\n",
    "                          num_epochs=1, batch_size=100,\n",
    "                          optimizer='adam',\n",
    "                          optim_config={\n",
    "                              'learning_rate': 1e-3,},\n",
    "                          temperature=temp, soft_target=average_softmax_15, distill_mode='proba',\n",
    "                          verbose=True, print_every=100)\n",
    "    tic = time.time()\n",
    "    small_model_15.train()\n",
    "    toc = time.time()\n",
    "    print('Execution time: ',toc-tic)\n",
    "    val_acc = small_model_15.check_accuracy(data['X_val'],data['y_val'])\n",
    "    print('Test accuracy: {}'.format(val_acc))\n",
    "    results[temp] =  val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('resultst25t50', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
