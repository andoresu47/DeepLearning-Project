{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import pickle\n",
    "from utils import load_mnist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):    \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_train = unpickle('logits_dir/resnet164_logits_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used in ResNet164\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train with small model\n",
    "data = {'X_train': x_train.reshape(len(x_train), 784).copy(), 'y_train': y_train,\n",
    "        'X_val': x_val.reshape(len(x_val), 784).copy(), 'y_val': y_val,\n",
    "        'X_test': x_test.reshape(len(x_test), 784).copy(), 'y_test': y_test,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel:\n",
    "    def __init__(self, \n",
    "                 model_type,\n",
    "                 num_steps=500, \n",
    "                 batch_size=128, \n",
    "                 display_step=100, \n",
    "                 n_hidden_1=256,\n",
    "                 n_hidden_2=256,\n",
    "                 num_input=784, \n",
    "                 num_classes=10,\n",
    "                 dropoutprob=0.75,\n",
    "                 checkpoint_dir=\"checkpoint\",\n",
    "                 checkpoint_file=\"smallmodel\",\n",
    "                 temperature=1.0,\n",
    "                 log_dir=\"logs\", \n",
    "                 learning_rate=0.001):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.display_step = display_step\n",
    "        self.n_hidden_1 = n_hidden_1  # 1st layer number of neurons\n",
    "        self.n_hidden_2 = n_hidden_2  # 2nd layer number of neurons\n",
    "        self.num_input = num_input  # MNIST data input (img shape: 28*28)\n",
    "        self.num_classes = num_classes\n",
    "        self.temperature = temperature\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_file)\n",
    "        self.max_checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_file + \"max\")\n",
    "        self.log_dir = os.path.join(log_dir, self.checkpoint_file)\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([self.num_input, self.n_hidden_1]),\n",
    "                              name=\"%s_%s\" % (self.model_type, \"h1\")),\n",
    "            'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2]),\n",
    "                              name=\"%s_%s\" % (self.model_type, \"h2\")),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.num_classes]),\n",
    "                               name=\"%s_%s\" % (self.model_type, \"out\")),\n",
    "            'linear': tf.Variable(tf.random_normal([self.num_input, self.num_classes]),\n",
    "                                  name=\"%s_%s\" % (self.model_type, \"linear\"))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden_1]), name=\"%s_%s\" % (self.model_type, \"b1\")),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden_2]), name=\"%s_%s\" % (self.model_type, \"b2\")),\n",
    "            'out': tf.Variable(tf.random_normal([self.num_classes]), name=\"%s_%s\" % (self.model_type, \"out\")),\n",
    "            'linear': tf.Variable(tf.random_normal([self.num_classes]), name=\"%s_%s\" % (self.model_type, \"linear\"))\n",
    "        }\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    # Create model\n",
    "    def build_model(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.num_input], name=\"%s_%s\" % (self.model_type, \"xinput\"))\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"%s_%s\" % (self.model_type, \"yinput\"))\n",
    "\n",
    "        self.flag = tf.placeholder(tf.bool, None, name=\"%s_%s\" % (self.model_type, \"flag\"))\n",
    "        self.soft_Y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"%s_%s\" % (self.model_type, \"softy\"))\n",
    "        self.softmax_temperature = tf.placeholder(tf.float32, name=\"%s_%s\" % (self.model_type, \"softmaxtemperature\"))\n",
    "\n",
    "        with tf.name_scope(\"%sfclayer\" % (self.model_type)), tf.variable_scope(\"%sfclayer\" % (self.model_type)):\n",
    "            # Hidden fully connected layer with 256 neurons\n",
    "            layer_1 = tf.add(tf.matmul(self.X, self.weights['h1']), self.biases['b1'])\n",
    "            # # Hidden fully connected layer with 256 neurons\n",
    "            layer_2 = tf.add(tf.matmul(layer_1, self.weights['h2']), self.biases['b2'])\n",
    "            # # Output fully connected layer with a neuron for each class\n",
    "            logits = (tf.matmul(layer_2, self.weights['out']) + self.biases['out'])\n",
    "            # logits = tf.add(tf.matmul(self.X, self.weights['linear']), self.biases['linear'])\n",
    "\n",
    "        with tf.name_scope(\"%sprediction\" % (self.model_type)), tf.variable_scope(\"%sprediction\" % (self.model_type)):\n",
    "            self.prediction = tf.nn.softmax(logits)\n",
    "\n",
    "            self.correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "\n",
    "        with tf.name_scope(\"%soptimization\" % (self.model_type)), tf.variable_scope(\n",
    "                        \"%soptimization\" % (self.model_type)):\n",
    "            # Define loss and optimizer\n",
    "            self.loss_op_standard = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=self.Y))\n",
    "\n",
    "            self.total_loss = self.loss_op_standard\n",
    "\n",
    "            self.loss_op_soft = tf.cond(self.flag,\n",
    "                                        true_fn=lambda: tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                            logits=logits / self.softmax_temperature, labels=self.soft_Y)),\n",
    "                                        false_fn=lambda: 0.0)\n",
    "\n",
    "            self.total_loss += tf.square(self.softmax_temperature) * self.loss_op_soft\n",
    "\n",
    "            # optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            # optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "            # self.global_step = tf.Variable(0, trainable=False)\n",
    "            # self.increment_global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "            # self.ad_learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "            #                                1000, 0.96, staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            # self.train_op = optimizer.minimize(self.total_loss, global_step=self.global_step)\n",
    "            self.train_op = optimizer.minimize(self.total_loss)\n",
    "\n",
    "        with tf.name_scope(\"%ssummarization\" % (self.model_type)), tf.variable_scope(\n",
    "                        \"%ssummarization\" % (self.model_type)):\n",
    "            tf.summary.scalar(\"loss_op_standard\", self.loss_op_standard)\n",
    "            tf.summary.scalar(\"total_loss\", self.total_loss)\n",
    "            # Create a summary to monitor accuracy tensor\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "            for var in tf.trainable_variables():\n",
    "                tf.summary.histogram(var.name, var)\n",
    "\n",
    "            # Merge all summaries into a single op\n",
    "\n",
    "            # If using TF 1.6 or above, simply use the following merge_all function\n",
    "            # which supports scoping\n",
    "            self.merged_summary_op = tf.summary.merge_all(scope=self.model_type)\n",
    "\n",
    "    def start_session(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n",
    "    def train(self, data, logits_distill=None):\n",
    "        teacher_flag = False\n",
    "        if logits_distill is not None:\n",
    "            teacher_flag = True\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "        X_val = data['X_val']\n",
    "        y_val = data['y_val']\n",
    "        \n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        train_summary_writer = tf.summary.FileWriter(self.log_dir, graph=self.sess.graph)\n",
    "\n",
    "        max_accuracy = 0\n",
    "\n",
    "        print(\"Starting Training\")\n",
    "\n",
    "        def dev_step():\n",
    "            validation_x = X_val\n",
    "            validation_y = y_val\n",
    "        \n",
    "            loss, acc = self.sess.run([self.loss_op_standard, self.accuracy], feed_dict={self.X: validation_x,\n",
    "                                                                                         self.Y: validation_y,\n",
    "                                                                                         # self.soft_Y: validation_y,\n",
    "                                                                                         self.flag: False,\n",
    "                                                                                         self.softmax_temperature: 1.0})\n",
    "\n",
    "            if acc > max_accuracy:\n",
    "                save_path = self.saver.save(self.sess, self.checkpoint_path)\n",
    "                print(\"Model Checkpointed to %s \" % (save_path))\n",
    "\n",
    "            print(\"Step \" + str(step) + \", Validation Loss= \" + \"{:.4f}\".format(\n",
    "                loss) + \", Validation Accuracy= \" + \"{:.3f}\".format(acc))\n",
    "\n",
    "        for step in range(1, self.num_steps + 1):\n",
    "            batch_x, batch_y, batch_logits = self.get_batch(X_train, y_train, logits_distill)\n",
    "            soft_targets = batch_y\n",
    "            if teacher_flag:\n",
    "                # soft_targets = self.sess.run(tf.nn.softmax(batch_logits / self.temperature))\n",
    "                soft_targets = batch_logits\n",
    "                \n",
    "            # self.sess.run(self.train_op,\n",
    "            _, summary = self.sess.run([self.train_op, self.merged_summary_op],\n",
    "                                       feed_dict={self.X: batch_x,\n",
    "                                                  self.Y: batch_y,\n",
    "                                                  self.soft_Y: soft_targets,\n",
    "                                                  self.flag: teacher_flag,\n",
    "                                                  self.softmax_temperature: self.temperature}\n",
    "                                       )\n",
    "            train_summary_writer.add_summary(summary, step)\n",
    "            \n",
    "            if (step % self.display_step) == 0 or step == 1:\n",
    "                dev_step()\n",
    "        else:\n",
    "            # Final Evaluation and checkpointing before training ends\n",
    "            dev_step()\n",
    "\n",
    "        train_summary_writer.close()\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "    def get_batch(self, X_train, y_train, logits_distill=None):\n",
    "        num_train = X_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = X_train[batch_mask]\n",
    "        y_batch = y_train[batch_mask]\n",
    "        if logits_distill is not None:\n",
    "            logit_batch = logits_distill[batch_mask]\n",
    "        else:\n",
    "            logit_batch = None\n",
    "        return X_batch, y_batch, logit_batch\n",
    "    \n",
    "    def predict(self, data_X, temperature=1.0):\n",
    "        return self.sess.run(self.prediction,\n",
    "                             feed_dict={self.X: data_X, self.flag: False, self.softmax_temperature: temperature})\n",
    "\n",
    "    def run_inference(self, data):   \n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        batch_size = self.batch_size\n",
    "        batch_num = int(len(X_test) / batch_size)\n",
    "        test_accuracy = 0\n",
    "\n",
    "        for i in range(batch_num):\n",
    "            batch_x = X_test[:batch_size * (i + 1)]\n",
    "            batch_y = y_test[:batch_size * (i + 1)]\n",
    "            test_accuracy += self.sess.run(self.accuracy, feed_dict={self.X: batch_x,\n",
    "                                                                     self.Y: batch_y,\n",
    "                                                                     self.flag: False,\n",
    "                                                                     self.softmax_temperature: 1.0\n",
    "                                                                    })\n",
    "        # test_images, test_labels = dataset.get_test_data()\n",
    "        # print(\"Testing Accuracy:\", self.sess.run(self.accuracy, feed_dict={self.X: test_images,\n",
    "                                                                           # self.Y: test_labels,\n",
    "                                                                           # # self.soft_Y: test_labels,\n",
    "                                                                           # self.flag: False,\n",
    "                                                                           # self.softmax_temperature: 1.0\n",
    "                                                                           # }))\n",
    "        test_accuracy /= batch_num\n",
    "        print(\"Testing Accuracy: %g\"%test_accuracy)\n",
    "        \n",
    "    def run_inference_ex(self, dataset_ex):\n",
    "        test_images, test_labels = dataset_ex.get_test_data_ex()\n",
    "        print(\"Testing Accuracy:\", self.sess.run(self.accuracy, feed_dict={self.X: test_images,\n",
    "                                                                           self.Y: test_labels,\n",
    "                                                                           # self.soft_Y: test_labels,\n",
    "                                                                           self.flag: False,\n",
    "                                                                           self.softmax_temperature: 1.0\n",
    "                                                                           }))\n",
    "\n",
    "    def load_model_from_file(self, load_path):\n",
    "        ckpt = tf.train.get_checkpoint_state(load_path)\n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters.\")\n",
    "            self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir=\"new_studentcpt_t1.75\"\n",
    "log_dir=checkpoint_dir + \"\\logs\"\n",
    "temperature = 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = StudentModel(num_steps=3000, \n",
    "                                 batch_size=128,\n",
    "                                 learning_rate=0.001,\n",
    "                                 temperature=temperature,\n",
    "                                 dropoutprob=0,\n",
    "                                 checkpoint_dir=checkpoint_dir,\n",
    "                                 log_dir=log_dir,\n",
    "                                 model_type=\"student\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speedup the calculations.\n",
    "session = tf.Session()\n",
    "soft_targets = session.run(tf.nn.softmax(logits_train / temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1, Validation Loss= 8475.0674, Validation Accuracy= 0.078\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 100, Validation Loss= 812.3453, Validation Accuracy= 0.768\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 200, Validation Loss= 547.8128, Validation Accuracy= 0.828\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 300, Validation Loss= 439.7054, Validation Accuracy= 0.851\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 400, Validation Loss= 385.6259, Validation Accuracy= 0.858\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 500, Validation Loss= 330.9241, Validation Accuracy= 0.868\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 600, Validation Loss= 297.5023, Validation Accuracy= 0.874\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 700, Validation Loss= 275.9644, Validation Accuracy= 0.875\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 800, Validation Loss= 260.2483, Validation Accuracy= 0.882\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 900, Validation Loss= 258.1494, Validation Accuracy= 0.879\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1000, Validation Loss= 237.2398, Validation Accuracy= 0.882\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1100, Validation Loss= 225.0993, Validation Accuracy= 0.883\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1200, Validation Loss= 222.3811, Validation Accuracy= 0.877\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1300, Validation Loss= 200.1690, Validation Accuracy= 0.888\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1400, Validation Loss= 200.9561, Validation Accuracy= 0.879\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1500, Validation Loss= 179.9883, Validation Accuracy= 0.889\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1600, Validation Loss= 178.0119, Validation Accuracy= 0.888\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1700, Validation Loss= 171.9655, Validation Accuracy= 0.885\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1800, Validation Loss= 169.5543, Validation Accuracy= 0.885\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 1900, Validation Loss= 184.6799, Validation Accuracy= 0.871\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2000, Validation Loss= 172.0526, Validation Accuracy= 0.879\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2100, Validation Loss= 157.6677, Validation Accuracy= 0.882\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2200, Validation Loss= 161.7039, Validation Accuracy= 0.883\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2300, Validation Loss= 160.7408, Validation Accuracy= 0.882\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2400, Validation Loss= 138.4895, Validation Accuracy= 0.891\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2500, Validation Loss= 138.7723, Validation Accuracy= 0.892\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2600, Validation Loss= 127.4156, Validation Accuracy= 0.898\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2700, Validation Loss= 129.0313, Validation Accuracy= 0.887\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2800, Validation Loss= 142.1558, Validation Accuracy= 0.873\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 2900, Validation Loss= 146.1089, Validation Accuracy= 0.876\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 3000, Validation Loss= 133.3567, Validation Accuracy= 0.880\n",
      "Model Checkpointed to new_studentcpt_t1.75\\smallmodel \n",
      "Step 3000, Validation Loss= 133.3567, Validation Accuracy= 0.880\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "student_model.start_session()\n",
    "student_model.train(data, soft_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from new_studentcpt_t1.75\\smallmodel\n",
      "Testing Accuracy: 0.857691\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from created checkpoint\n",
    "student_model.load_model_from_file(checkpoint_dir)\n",
    "# Test the model against the testing set\n",
    "student_model.run_inference(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close current tf sessions\n",
    "student_model.close_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.2030087 , -3.9134192 , -6.317644  ,  6.6796656 , -2.957327  ,\n",
       "       12.809154  , -3.4207807 ,  0.18087192,  2.3757007 ,  0.04373608],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0., -1., 2., 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "soft_targets = session.run(tf.nn.softmax(logits_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model_pred = np.argmax(session.run(tf.nn.softmax(logits_train)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.argmax(y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99966"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(big_model_pred == test_pred) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
