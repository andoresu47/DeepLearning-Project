{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilling the knowledge in a Neural Network: Tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_train_logits = np.load(\n",
    "    'data-lables-logits/new_teacher_train_logits.npz')['arr_0']\n",
    "teacher_val_logits = np.load(\n",
    "    'data-lables-logits/new_teacher_val_logits.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.load('data-lables-logits/x_val.npz')['arr_0']\n",
    "x_train = np.load('data-lables-logits/new_x_train.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('data-lables-logits/new_y_train.npz')['arr_0']\n",
    "y_val = np.load('data-lables-logits/y_val.npz')['arr_0']\n",
    "#one hot representation\n",
    "y_train = to_categorical(y_train)\n",
    "y_val= to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train with small model\n",
    "data = {'X_train': x_train.transpose(0,3,1,2).copy(), 'y_train': np.argmax(y_train,axis=1),\n",
    "        'X_val': x_val.transpose(0,3,1,2).copy(), 'y_val': np.argmax(y_val,axis=1),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 70.010633\n",
      "(Epoch 0 / 1) train acc: 0.005093; val_acc: 0.005085\n",
      "(Iteration 101 / 981) loss: 69.167259\n",
      "(Iteration 201 / 981) loss: 68.645511\n",
      "(Iteration 301 / 981) loss: 68.642078\n",
      "(Iteration 401 / 981) loss: 68.543822\n",
      "(Iteration 501 / 981) loss: 68.256471\n",
      "(Iteration 601 / 981) loss: 68.085401\n",
      "(Iteration 701 / 981) loss: 68.088267\n",
      "(Iteration 801 / 981) loss: 68.050134\n",
      "(Iteration 901 / 981) loss: 68.099110\n",
      "(Epoch 1 / 1) train acc: 0.165952; val_acc: 0.124288\n",
      "Execution time:  10451.32723402977\n"
     ]
    }
   ],
   "source": [
    "# Train small model with distilling\n",
    "net = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=56,filter_size=5,hidden_dim=800,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model = myModel(net, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=5.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing convNet architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 14.789422\n",
      "(Epoch 0 / 1) train acc: 0.006081; val_acc: 0.006204\n",
      "(Iteration 101 / 981) loss: 13.518807\n",
      "(Iteration 201 / 981) loss: 13.053691\n",
      "(Iteration 301 / 981) loss: 12.456631\n",
      "(Iteration 401 / 981) loss: 12.169530\n",
      "(Iteration 501 / 981) loss: 12.356519\n",
      "(Iteration 601 / 981) loss: 12.292561\n",
      "(Iteration 701 / 981) loss: 12.359798\n",
      "(Iteration 801 / 981) loss: 12.151385\n",
      "(Iteration 901 / 981) loss: 11.999670\n",
      "(Epoch 1 / 1) train acc: 0.156449; val_acc: 0.128356\n",
      "Execution time:  7263.9244928359985\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net2 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model2 = myModel(net2, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=2.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model2.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 27.852502\n",
      "(Epoch 0 / 1) train acc: 0.005032; val_acc: 0.004984\n",
      "(Iteration 101 / 981) loss: 26.468740\n",
      "(Iteration 201 / 981) loss: 26.019814\n",
      "(Iteration 301 / 981) loss: 25.969969\n",
      "(Iteration 401 / 981) loss: 25.719305\n",
      "(Iteration 501 / 981) loss: 25.421016\n",
      "(Iteration 601 / 981) loss: 25.223293\n",
      "(Iteration 701 / 981) loss: 25.499452\n",
      "(Iteration 801 / 981) loss: 25.474299\n",
      "(Iteration 901 / 981) loss: 25.351175\n",
      "(Epoch 1 / 1) train acc: 0.165229; val_acc: 0.138222\n",
      "Execution time:  6585.098785638809\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net3 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model3 = myModel(net3, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=3.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model3.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 46.377089\n",
      "(Epoch 0 / 1) train acc: 0.005449; val_acc: 0.005492\n",
      "(Iteration 101 / 981) loss: 44.997808\n",
      "(Iteration 201 / 981) loss: 44.483580\n",
      "(Iteration 301 / 981) loss: 44.345702\n",
      "(Iteration 401 / 981) loss: 44.334093\n",
      "(Iteration 501 / 981) loss: 44.213389\n",
      "(Iteration 601 / 981) loss: 44.115374\n",
      "(Iteration 701 / 981) loss: 44.114152\n",
      "(Iteration 801 / 981) loss: 44.213968\n",
      "(Iteration 901 / 981) loss: 44.075203\n",
      "(Epoch 1 / 1) train acc: 0.162968; val_acc: 0.138426\n",
      "Execution time:  8200.785106897354\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net4 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model4 = myModel(net4, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=4.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model4.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 70.327320\n",
      "(Epoch 0 / 1) train acc: 0.006702; val_acc: 0.007628\n",
      "(Iteration 101 / 981) loss: 69.122638\n",
      "(Iteration 201 / 981) loss: 68.596730\n",
      "(Iteration 301 / 981) loss: 68.412241\n",
      "(Iteration 401 / 981) loss: 68.398369\n",
      "(Iteration 501 / 981) loss: 68.355414\n",
      "(Iteration 601 / 981) loss: 68.110488\n",
      "(Iteration 701 / 981) loss: 67.969043\n",
      "(Iteration 801 / 981) loss: 67.930572\n",
      "(Iteration 901 / 981) loss: 67.870707\n",
      "(Epoch 1 / 1) train acc: 0.154646; val_acc: 0.127644\n",
      "Execution time:  7831.729970932007\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net2 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model2 = myModel(net2, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=5.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model2.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 1962) loss: 70.293342\n",
      "(Epoch 0 / 2) train acc: 0.005133; val_acc: 0.005696\n",
      "(Iteration 101 / 1962) loss: 69.066719\n",
      "(Iteration 201 / 1962) loss: 68.776389\n",
      "(Iteration 301 / 1962) loss: 68.618383\n",
      "(Iteration 401 / 1962) loss: 68.675209\n",
      "(Iteration 501 / 1962) loss: 68.196481\n",
      "(Iteration 601 / 1962) loss: 68.189083\n",
      "(Iteration 701 / 1962) loss: 67.988023\n",
      "(Iteration 801 / 1962) loss: 68.024110\n",
      "(Iteration 901 / 1962) loss: 67.831734\n",
      "(Epoch 1 / 2) train acc: 0.148260; val_acc: 0.128865\n",
      "(Iteration 1001 / 1962) loss: 68.198674\n",
      "(Iteration 1101 / 1962) loss: 68.047031\n",
      "(Iteration 1201 / 1962) loss: 67.907314\n",
      "(Iteration 1301 / 1962) loss: 67.592077\n",
      "(Iteration 1401 / 1962) loss: 67.770902\n",
      "(Iteration 1501 / 1962) loss: 67.518414\n",
      "(Iteration 1601 / 1962) loss: 67.791943\n",
      "(Iteration 1701 / 1962) loss: 67.850577\n",
      "(Iteration 1801 / 1962) loss: 67.919487\n",
      "(Iteration 1901 / 1962) loss: 67.504108\n",
      "(Epoch 2 / 2) train acc: 0.189542; val_acc: 0.160191\n",
      "Execution time:  22749.209285736084\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net3 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model3 = myModel(net3, data, \n",
    "                      num_epochs=2, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=5.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model3.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 133.787263\n",
      "(Epoch 0 / 1) train acc: 0.005836; val_acc: 0.005289\n",
      "(Iteration 101 / 981) loss: 132.493170\n",
      "(Iteration 201 / 981) loss: 131.967544\n",
      "(Iteration 301 / 981) loss: 131.706930\n",
      "(Iteration 401 / 981) loss: 131.884901\n",
      "(Iteration 501 / 981) loss: 131.699092\n",
      "(Iteration 601 / 981) loss: 131.611450\n",
      "(Iteration 701 / 981) loss: 131.588350\n",
      "(Iteration 801 / 981) loss: 131.604190\n",
      "(Iteration 901 / 981) loss: 131.569146\n",
      "(Epoch 1 / 1) train acc: 0.150755; val_acc: 0.128356\n",
      "Execution time:  10058.536261081696\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net7 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model7 = myModel(net7, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=7.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model7.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 218.717686\n",
      "(Epoch 0 / 1) train acc: 0.004909; val_acc: 0.004984\n",
      "(Iteration 101 / 981) loss: 217.479674\n",
      "(Iteration 201 / 981) loss: 216.968641\n",
      "(Iteration 301 / 981) loss: 216.779499\n",
      "(Iteration 401 / 981) loss: 216.508255\n",
      "(Iteration 501 / 981) loss: 216.564372\n",
      "(Iteration 601 / 981) loss: 216.532938\n",
      "(Iteration 701 / 981) loss: 216.525283\n",
      "(Iteration 801 / 981) loss: 216.282119\n",
      "(Iteration 901 / 981) loss: 216.446858\n",
      "(Epoch 1 / 1) train acc: 0.152497; val_acc: 0.129170\n",
      "Execution time:  6565.998723745346\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net9 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model9 = myModel(net9, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=9.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model9.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 600.055094\n",
      "(Epoch 0 / 1) train acc: 0.007843; val_acc: 0.007018\n",
      "(Iteration 101 / 981) loss: 598.720700\n",
      "(Iteration 201 / 981) loss: 598.443015\n",
      "(Iteration 301 / 981) loss: 598.259757\n",
      "(Iteration 401 / 981) loss: 598.183518\n",
      "(Iteration 501 / 981) loss: 597.949177\n",
      "(Iteration 601 / 981) loss: 597.862653\n",
      "(Iteration 701 / 981) loss: 597.957160\n",
      "(Iteration 801 / 981) loss: 597.895342\n",
      "(Iteration 901 / 981) loss: 597.833059\n",
      "(Epoch 1 / 1) train acc: 0.136984; val_acc: 0.120118\n",
      "Execution time:  6979.847075223923\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net15 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model15 = myModel(net15, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=15.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model15.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 1063.758219\n",
      "(Epoch 0 / 1) train acc: 0.005184; val_acc: 0.005492\n",
      "(Iteration 101 / 981) loss: 1062.461154\n",
      "(Iteration 201 / 981) loss: 1062.134165\n",
      "(Iteration 301 / 981) loss: 1061.846332\n",
      "(Iteration 401 / 981) loss: 1061.796098\n",
      "(Iteration 501 / 981) loss: 1061.768990\n",
      "(Iteration 601 / 981) loss: 1061.690525\n",
      "(Iteration 701 / 981) loss: 1061.503399\n",
      "(Iteration 801 / 981) loss: 1061.515169\n",
      "(Iteration 901 / 981) loss: 1061.502553\n",
      "(Epoch 1 / 1) train acc: 0.148199; val_acc: 0.128560\n",
      "Execution time:  11017.23688197136\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net20 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model20 = myModel(net20, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=20.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model20.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 981) loss: 6626.853085\n",
      "(Epoch 0 / 1) train acc: 0.007639; val_acc: 0.007628\n",
      "(Iteration 101 / 981) loss: 6625.524839\n",
      "(Iteration 201 / 981) loss: 6625.408788\n",
      "(Iteration 301 / 981) loss: 6625.175543\n",
      "(Iteration 401 / 981) loss: 6624.919692\n",
      "(Iteration 501 / 981) loss: 6624.874785\n",
      "(Iteration 601 / 981) loss: 6624.760851\n",
      "(Iteration 701 / 981) loss: 6624.873809\n",
      "(Iteration 801 / 981) loss: 6624.746782\n",
      "(Iteration 901 / 981) loss: 6624.722949\n",
      "(Epoch 1 / 1) train acc: 0.147700; val_acc: 0.125407\n",
      "Execution time:  6589.847578048706\n"
     ]
    }
   ],
   "source": [
    "# Another test Train small model with distilling\n",
    "net50 = ThreeLayerConvNet(input_dim=(3, 56, 56),num_classes=200,num_filters=16,filter_size=7,hidden_dim=1024,\n",
    "                        reg=0.001,weight_scale=1,dtype=np.float32)\n",
    "small_model50 = myModel(net50, data, \n",
    "                      num_epochs=1, batch_size=100,\n",
    "                      optimizer='adam',\n",
    "                      optim_config={\n",
    "                          'learning_rate': 1e-3,},\n",
    "                      temperature=50.0,logit_distill=teacher_train_logits,\n",
    "                      verbose=True, print_every=100)\n",
    "tic = time.time()\n",
    "small_model50.train()\n",
    "toc = time.time()\n",
    "print('Execution time: ',toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
