{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy.core.multiarray\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling Knowledge Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Softmax layer**: Referrs to the output layer, the \"softmax\" converts the logit (Wx + b) computed for each class into a probability. This means that the output of the network for each image will be a probability for each of the classes. \n",
    "\n",
    "\n",
    "**Pameter T :** Temperature, when we set a low temperature, the probability will be discrete and we will make confident predictions, if we increase the temperature it will smooth and converge towards one over the number of classes, we can set the temperature to 1 so we can increase the temperature and therefore increase the robustness of the model.\n",
    "\n",
    "In the training data, we can raise the temperature to a large value, 40-50, with this we will produce vectors that are very smooth, we then use this probability vectors to label the data. We replace the labels and train the same architecture using this new labeled data set and obtain a new model: the distilled model. \n",
    "\n",
    "To make predictions using this new model we have to set the temperature back to 1, a test time this way we will still have discrete confident predictions.  when models are trained they are typically trained to minimized a loss function.\n",
    "\n",
    "\n",
    "yields model smoothness-- jacobian\n",
    "easy implementation, \n",
    "acceptable impact on accuracy\n",
    "\n",
    "adversarial examples\n",
    "evaluate robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up google cloud platform**\n",
    "\n",
    "\n",
    "1. created the project\n",
    "2. added the billing straight from the confirmation email with the grant activation code.\n",
    "3. linked the billing to the project\n",
    "4. Zones\n",
    "5. Enabled billing\n",
    "6. Edit quota. (sent a request for increasing GPUs to 1)\n",
    "Then you can create a deep learning VM which I didn't as I think once you deploy it, it starts counting on your budget. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Tried, to play with tensor flow and understad how we can model NN using keras.\n",
    "\n",
    "**2.** Based on what Nga shared, https://github.com/Curt-Park/handwritten_digit_recognition/blob/master/images/ResNet164.png I loaded the pre-trained ResNet164.h5 model: For building the distilled model, we need to get the weights prior computing the softmax, so I tried to modify the function ResNet164() so we can get the plain \"y\" instead of the softmax. **It worked**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distilling:**\n",
    "1. compute output probabities, for this we use the softmax but we introduce the **temperature** term. When this is 0 the output probabilities will be the same as the softmax. usefule link for understanding the temperature hyperparameter : \n",
    " * https://towardsdatascience.com/knowledge-distillation-and-the-concept-of-dark-knowledge-8b7aed8014ac.\n",
    "\n",
    "\" The main idea behind the knowledge distillation is transferring this dark knowledge from a well trained teacher to a lighter student model.\"\n",
    "\n",
    "2. We need to compute the **weighted average of the loss functions**: cross entropy with soft targets and cross entropy with the correct labels. We can do this creating a new loss function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
